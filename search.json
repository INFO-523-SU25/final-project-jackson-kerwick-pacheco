[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Urbanization and Environmental Quality in Arizona, USA",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport glob\nThe goal of our project is to analyze the relationship between urbanization and environmental quality across regions within Arizona, identifying patterns and anomalies in how urban growth impacts climate-related factors such as storms, temperature, and drought"
  },
  {
    "objectID": "proposal.html#datasets",
    "href": "proposal.html#datasets",
    "title": "Urbanization and Environmental Quality in Arizona, USA",
    "section": "Datasets",
    "text": "Datasets\n\nDataset 1 - Storm Data in Arizona\n\n# Storm Events in Arizona\n\n# Load csv file\ndf = pd.read_csv('data/StormEvents/storm_data_search_results.csv')\n\n# Print using shape property\nprint(\"Shape:\", df.shape)\n\n# Print column names\nprint(\"Columns:\", df.columns.tolist())\n\n# Print data types of all of the columns\nprint(\"Data types:\", df.dtypes)\n\n# Print summary of the data\ndf.info()\n\nShape: (500, 39)\nColumns: ['EVENT_ID', 'CZ_NAME_STR', 'BEGIN_LOCATION', 'BEGIN_DATE', 'BEGIN_TIME', 'EVENT_TYPE', 'MAGNITUDE', 'TOR_F_SCALE', 'DEATHS_DIRECT', 'INJURIES_DIRECT', 'DAMAGE_PROPERTY_NUM', 'DAMAGE_CROPS_NUM', 'STATE_ABBR', 'CZ_TIMEZONE', 'MAGNITUDE_TYPE', 'EPISODE_ID', 'CZ_TYPE', 'CZ_FIPS', 'WFO', 'INJURIES_INDIRECT', 'DEATHS_INDIRECT', 'SOURCE', 'FLOOD_CAUSE', 'TOR_LENGTH', 'TOR_WIDTH', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'END_DATE', 'END_TIME', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'EVENT_NARRATIVE', 'EPISODE_NARRATIVE', 'ABSOLUTE_ROWNUMBER']\nData types: EVENT_ID                int64\nCZ_NAME_STR            object\nBEGIN_LOCATION         object\nBEGIN_DATE             object\nBEGIN_TIME              int64\nEVENT_TYPE             object\nMAGNITUDE              object\nTOR_F_SCALE            object\nDEATHS_DIRECT           int64\nINJURIES_DIRECT         int64\nDAMAGE_PROPERTY_NUM     int64\nDAMAGE_CROPS_NUM        int64\nSTATE_ABBR             object\nCZ_TIMEZONE            object\nMAGNITUDE_TYPE         object\nEPISODE_ID              int64\nCZ_TYPE                object\nCZ_FIPS                 int64\nWFO                    object\nINJURIES_INDIRECT       int64\nDEATHS_INDIRECT         int64\nSOURCE                 object\nFLOOD_CAUSE            object\nTOR_LENGTH             object\nTOR_WIDTH              object\nBEGIN_RANGE            object\nBEGIN_AZIMUTH          object\nEND_RANGE              object\nEND_AZIMUTH            object\nEND_LOCATION           object\nEND_DATE               object\nEND_TIME                int64\nBEGIN_LAT              object\nBEGIN_LON              object\nEND_LAT                object\nEND_LON                object\nEVENT_NARRATIVE        object\nEPISODE_NARRATIVE      object\nABSOLUTE_ROWNUMBER      int64\ndtype: object\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 39 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   EVENT_ID             500 non-null    int64 \n 1   CZ_NAME_STR          500 non-null    object\n 2   BEGIN_LOCATION       500 non-null    object\n 3   BEGIN_DATE           500 non-null    object\n 4   BEGIN_TIME           500 non-null    int64 \n 5   EVENT_TYPE           500 non-null    object\n 6   MAGNITUDE            500 non-null    object\n 7   TOR_F_SCALE          500 non-null    object\n 8   DEATHS_DIRECT        500 non-null    int64 \n 9   INJURIES_DIRECT      500 non-null    int64 \n 10  DAMAGE_PROPERTY_NUM  500 non-null    int64 \n 11  DAMAGE_CROPS_NUM     500 non-null    int64 \n 12  STATE_ABBR           500 non-null    object\n 13  CZ_TIMEZONE          500 non-null    object\n 14  MAGNITUDE_TYPE       500 non-null    object\n 15  EPISODE_ID           500 non-null    int64 \n 16  CZ_TYPE              500 non-null    object\n 17  CZ_FIPS              500 non-null    int64 \n 18  WFO                  500 non-null    object\n 19  INJURIES_INDIRECT    500 non-null    int64 \n 20  DEATHS_INDIRECT      500 non-null    int64 \n 21  SOURCE               500 non-null    object\n 22  FLOOD_CAUSE          500 non-null    object\n 23  TOR_LENGTH           500 non-null    object\n 24  TOR_WIDTH            500 non-null    object\n 25  BEGIN_RANGE          500 non-null    object\n 26  BEGIN_AZIMUTH        500 non-null    object\n 27  END_RANGE            500 non-null    object\n 28  END_AZIMUTH          500 non-null    object\n 29  END_LOCATION         500 non-null    object\n 30  END_DATE             500 non-null    object\n 31  END_TIME             500 non-null    int64 \n 32  BEGIN_LAT            500 non-null    object\n 33  BEGIN_LON            500 non-null    object\n 34  END_LAT              500 non-null    object\n 35  END_LON              500 non-null    object\n 36  EVENT_NARRATIVE      356 non-null    object\n 37  EPISODE_NARRATIVE    500 non-null    object\n 38  ABSOLUTE_ROWNUMBER   500 non-null    int64 \ndtypes: int64(12), object(27)\nmemory usage: 152.5+ KB\n\n\nThis dataset contains storm event records in Arizona, sourced from the NOAA Storm Events Database. It includes information about various weather events such as floods, tornadoes, and severe storms, along with details like location, date, event type, magnitude, fatalities, injuries, and property damage. It also includes metadata such as time zones, county information, and narrative descriptions of each event.\nThe dataset consists of a mix of numerical and categorical values, specifically, 12 columns with integer types and 27 with object types. It was chosen for its relevance to climate and environmental analysis in Arizona. The data enables the exploration of temporal and spatial patterns in extreme weather events and supports investigations into trends related to climate change, urbanization, and risk assessment.\n\n\nDataset 2 - Weather Data in Arizona\n\n# NOAA Data\n\n# reading in data and\nfile_location = \"data/NOAA/\"\nall_files = glob.glob(file_location + \"*.csv\")\n\nlist_of_dfs = []\n\nfor file in all_files:\n  df = pd.read_csv(file)\n  list_of_dfs.append(df)\n\n# merging data together into one data frame\nnoaadata = pd.concat(list_of_dfs, ignore_index = True)\n\n\n# glance at NOAA climate data\n\n# Print using shape property\nprint(\"Shape:\", noaadata.shape)\n\n# Print column names\nprint(\"Columns:\", noaadata.columns.tolist())\n\n# Print data types of all of the columns\nprint(\"Data types:\", noaadata.dtypes)\n\n# Print summary of the data\nnoaadata.info()\n\nnoaadata.head()\n\n# scatterplot of select variables to test\nsns.scatterplot(data = noaadata, x = \"PRCP\", y = \"AWND\")\nplt.title(\"Example plot: Relationship between precipitation and wind speed\")\nplt.xlabel(\" Precipitation\")\nplt.ylabel(\"Average Wind Speed\")\nplt.show()\n\nShape: (1261660, 37)\nColumns: ['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'DATE', 'AWND', 'DAPR', 'DASF', 'EVAP', 'MDPR', 'MDSF', 'PGTM', 'PRCP', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TOBS', 'WDF2', 'WDF5', 'WESD', 'WESF', 'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11']\nData types: STATION       object\nNAME          object\nLATITUDE     float64\nLONGITUDE    float64\nELEVATION    float64\nDATE          object\nAWND         float64\nDAPR         float64\nDASF         float64\nEVAP         float64\nMDPR         float64\nMDSF         float64\nPGTM         float64\nPRCP         float64\nSNOW         float64\nSNWD         float64\nTAVG         float64\nTMAX         float64\nTMIN         float64\nTOBS         float64\nWDF2         float64\nWDF5         float64\nWESD         float64\nWESF         float64\nWSF2         float64\nWSF5         float64\nWT01         float64\nWT02         float64\nWT03         float64\nWT04         float64\nWT05         float64\nWT06         float64\nWT07         float64\nWT08         float64\nWT09         float64\nWT10         float64\nWT11         float64\ndtype: object\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1261660 entries, 0 to 1261659\nData columns (total 37 columns):\n #   Column     Non-Null Count    Dtype  \n---  ------     --------------    -----  \n 0   STATION    1261660 non-null  object \n 1   NAME       1159411 non-null  object \n 2   LATITUDE   1261660 non-null  float64\n 3   LONGITUDE  1261660 non-null  float64\n 4   ELEVATION  1259522 non-null  float64\n 5   DATE       1261660 non-null  object \n 6   AWND       34239 non-null    float64\n 7   DAPR       5504 non-null     float64\n 8   DASF       1 non-null        float64\n 9   EVAP       1968 non-null     float64\n 10  MDPR       5448 non-null     float64\n 11  MDSF       1 non-null        float64\n 12  PGTM       2341 non-null     float64\n 13  PRCP       1102560 non-null  float64\n 14  SNOW       763234 non-null   float64\n 15  SNWD       224120 non-null   float64\n 16  TAVG       197007 non-null   float64\n 17  TMAX       437529 non-null   float64\n 18  TMIN       436673 non-null   float64\n 19  TOBS       218761 non-null   float64\n 20  WDF2       34278 non-null    float64\n 21  WDF5       34174 non-null    float64\n 22  WESD       49066 non-null    float64\n 23  WESF       8717 non-null     float64\n 24  WSF2       34279 non-null    float64\n 25  WSF5       34175 non-null    float64\n 26  WT01       3758 non-null     float64\n 27  WT02       534 non-null      float64\n 28  WT03       5357 non-null     float64\n 29  WT04       219 non-null      float64\n 30  WT05       233 non-null      float64\n 31  WT06       40 non-null       float64\n 32  WT07       85 non-null       float64\n 33  WT08       3046 non-null     float64\n 34  WT09       7 non-null        float64\n 35  WT10       2 non-null        float64\n 36  WT11       333 non-null      float64\ndtypes: float64(34), object(3)\nmemory usage: 356.2+ MB\n\n\n\n\n\n\n\n\n\nThis data from [NOAA National Centers for Environmental Information] (https://www.ncdc.noaa.gov/cdo-web/datasets) will serve as a measure of environmental quality. The dataset noaadata is a compilation of daily land surface observations within Arizona from 2018 to 2023. Some variables of importance includes latitude and longitude of station, temperatures, precipitation, and snowfall.\nThis data contains primarily numerical values, with the only categorical variables being the name of the station where weather data is collected, and date the data was collected.\n\n\nDataset 3 - Traffic Data in Arizona\n\n# Traffic Station Data \n\n# Traffic data sourced from FHWA with aggregate station data over all \n# months from 2019 to 2023. FWHA changed their data format in 2022, so \n# the data are stored in two different formats\n\n# storing each year of traffic data as a dictionary of pd.DataFrames\nstation_data = {}\n\nfor year in np.arange(2019,2024):\n  df = pd.DataFrame()\n  if (year &lt; 2022):   \n    # old data format\n    fname = f'data/Traffic/stations/Station_Data_Extract_Pipe_Delimited_CleanData_{str(year)}.txt'\n    df = pd.read_csv(fname, sep='|', low_memory=False)\n    # For old format only, remove every entry where state_code != 04 (entries not in Arizona)\n    df = df[(df['State_Code'] == 4)]\n    df.columns = [x.lower() for x in df.columns]  # make columns lowercase like new data format\n  else:\n    # new data format\n    df = pd.read_csv('data/Traffic/stations/AZ_2023 (TMAS).txt', sep='|')\n\n  df = df.reset_index()\n  station_data[year] = df\n\n\n# Information about station dataset\n\nprint(\"Geographic locations of each station are formatted as follows:\")\nprint(station_data[2019].columns)\nprint(station_data[2019].info())\n\n\n# Traffic counts at each station are in files. All station and traffic data\n# will be aggregated together into a dataframe where each row is a traffic\n# count for each year for each county \n\n# data conversion tool https://github.com/policyinfo/TMAS-Traffic-Volume-Data-Rearrangement-Tool/blob/main/Tool_Document.pdf\n\ntraffic_files = []\n\nfor i in range(19, 23):\n  year = \"{:02}\".format(i)\n  for j in range(1,13):\n    monthyear = \"{:02}\".format(j) + year\n    fname = f\"data/Traffic/counts/AZ{monthyear}.VOL\"\n    traffic_files.append(fname)\n\nprint(\"Counts of traffic stations will be read in from the following files:\")\nprint(traffic_files)\n\nGeographic locations of each station are formatted as follows:\nIndex(['index', 'record_type', 'state_code', 'station_id', 'travel_dir',\n       'travel_lane', 'year_record', 'f_system', 'num_lanes',\n       'sample_type_volume', 'num_lanes_volume', 'method_volume',\n       'sample_type_class', 'num_lanes_class', 'method_class',\n       'algorithm_volume', 'num_classes', 'sample_type_truck',\n       'num_lanes_truck', 'method_truck', 'calibration', 'data_retrieval',\n       'type_sensor_1', 'type_sensor_2', 'primary_purpose', 'lrs_id',\n       'lrs_point', 'latitude', 'longitude', 'shrp_id', 'prev_station_id',\n       'year_established', 'year_discontinued', 'county_code', 'is_sample',\n       'sample_id', 'nhs', 'posted_route_signing', 'posted_signed_route',\n       'con_route_signing', 'con_signed_route', 'station_location'],\n      dtype='object')\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2000 entries, 0 to 1999\nData columns (total 42 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   index                 2000 non-null   int64 \n 1   record_type           2000 non-null   object\n 2   state_code            2000 non-null   int64 \n 3   station_id            2000 non-null   object\n 4   travel_dir            2000 non-null   int64 \n 5   travel_lane           2000 non-null   int64 \n 6   year_record           2000 non-null   int64 \n 7   f_system              2000 non-null   object\n 8   num_lanes             2000 non-null   int64 \n 9   sample_type_volume    2000 non-null   object\n 10  num_lanes_volume      2000 non-null   int64 \n 11  method_volume         2000 non-null   int64 \n 12  sample_type_class     2000 non-null   object\n 13  num_lanes_class       2000 non-null   int64 \n 14  method_class          2000 non-null   int64 \n 15  algorithm_volume      2000 non-null   object\n 16  num_classes           2000 non-null   int64 \n 17  sample_type_truck     2000 non-null   object\n 18  num_lanes_truck       2000 non-null   int64 \n 19  method_truck          2000 non-null   int64 \n 20  calibration           2000 non-null   object\n 21  data_retrieval        2000 non-null   int64 \n 22  type_sensor_1         2000 non-null   object\n 23  type_sensor_2         2000 non-null   object\n 24  primary_purpose       2000 non-null   object\n 25  lrs_id                2000 non-null   object\n 26  lrs_point             2000 non-null   object\n 27  latitude              2000 non-null   int64 \n 28  longitude             2000 non-null   int64 \n 29  shrp_id               2000 non-null   object\n 30  prev_station_id       2000 non-null   object\n 31  year_established      2000 non-null   object\n 32  year_discontinued     2000 non-null   object\n 33  county_code           2000 non-null   int64 \n 34  is_sample             2000 non-null   object\n 35  sample_id             2000 non-null   object\n 36  nhs                   2000 non-null   object\n 37  posted_route_signing  2000 non-null   int64 \n 38  posted_signed_route   2000 non-null   object\n 39  con_route_signing     2000 non-null   int64 \n 40  con_signed_route      2000 non-null   object\n 41  station_location      2000 non-null   object\ndtypes: int64(19), object(23)\nmemory usage: 656.4+ KB\nNone\nCounts of traffic stations will be read in from the following files:\n['data/Traffic/counts/AZ0119.VOL', 'data/Traffic/counts/AZ0219.VOL', 'data/Traffic/counts/AZ0319.VOL', 'data/Traffic/counts/AZ0419.VOL', 'data/Traffic/counts/AZ0519.VOL', 'data/Traffic/counts/AZ0619.VOL', 'data/Traffic/counts/AZ0719.VOL', 'data/Traffic/counts/AZ0819.VOL', 'data/Traffic/counts/AZ0919.VOL', 'data/Traffic/counts/AZ1019.VOL', 'data/Traffic/counts/AZ1119.VOL', 'data/Traffic/counts/AZ1219.VOL', 'data/Traffic/counts/AZ0120.VOL', 'data/Traffic/counts/AZ0220.VOL', 'data/Traffic/counts/AZ0320.VOL', 'data/Traffic/counts/AZ0420.VOL', 'data/Traffic/counts/AZ0520.VOL', 'data/Traffic/counts/AZ0620.VOL', 'data/Traffic/counts/AZ0720.VOL', 'data/Traffic/counts/AZ0820.VOL', 'data/Traffic/counts/AZ0920.VOL', 'data/Traffic/counts/AZ1020.VOL', 'data/Traffic/counts/AZ1120.VOL', 'data/Traffic/counts/AZ1220.VOL', 'data/Traffic/counts/AZ0121.VOL', 'data/Traffic/counts/AZ0221.VOL', 'data/Traffic/counts/AZ0321.VOL', 'data/Traffic/counts/AZ0421.VOL', 'data/Traffic/counts/AZ0521.VOL', 'data/Traffic/counts/AZ0621.VOL', 'data/Traffic/counts/AZ0721.VOL', 'data/Traffic/counts/AZ0821.VOL', 'data/Traffic/counts/AZ0921.VOL', 'data/Traffic/counts/AZ1021.VOL', 'data/Traffic/counts/AZ1121.VOL', 'data/Traffic/counts/AZ1221.VOL', 'data/Traffic/counts/AZ0122.VOL', 'data/Traffic/counts/AZ0222.VOL', 'data/Traffic/counts/AZ0322.VOL', 'data/Traffic/counts/AZ0422.VOL', 'data/Traffic/counts/AZ0522.VOL', 'data/Traffic/counts/AZ0622.VOL', 'data/Traffic/counts/AZ0722.VOL', 'data/Traffic/counts/AZ0822.VOL', 'data/Traffic/counts/AZ0922.VOL', 'data/Traffic/counts/AZ1022.VOL', 'data/Traffic/counts/AZ1122.VOL', 'data/Traffic/counts/AZ1222.VOL']\n\n\n\n# As an example, here is traffic count data from a single month of a single year\ndf_count = pd.read_csv('data/Traffic/counts/AZ1019.VOL')\nprint(df_count.columns)\nprint(df_count.info())\n\n# The following is an example of how we will combine hourly counts into a sum over each day\n\ndf_volumes = df_count.copy()\ndf_volumes['daily_volume'] = df_volumes[[\"Hour_{:02}\".format(i) for i in range(24)]].sum(axis=1)\ndf_volumes = df_volumes.drop([\"Hour_{:02}\".format(i) for i in range(24)], axis=1)\n\nIndex(['Record_Type', 'State_Code', 'F_System', 'Station_Id', 'Travel_Dir',\n       'Travel_Lane', 'Year_Record', 'Month_Record', 'Day_Record',\n       'Day_of_Week', 'Hour_00', 'Hour_01', 'Hour_02', 'Hour_03', 'Hour_04',\n       'Hour_05', 'Hour_06', 'Hour_07', 'Hour_08', 'Hour_09', 'Hour_10',\n       'Hour_11', 'Hour_12', 'Hour_13', 'Hour_14', 'Hour_15', 'Hour_16',\n       'Hour_17', 'Hour_18', 'Hour_19', 'Hour_20', 'Hour_21', 'Hour_22',\n       'Hour_23', 'Restrictions'],\n      dtype='object')\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21866 entries, 0 to 21865\nData columns (total 35 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Record_Type   21866 non-null  int64 \n 1   State_Code    21866 non-null  int64 \n 2   F_System      21866 non-null  object\n 3   Station_Id    21866 non-null  int64 \n 4   Travel_Dir    21866 non-null  int64 \n 5   Travel_Lane   21866 non-null  int64 \n 6   Year_Record   21866 non-null  int64 \n 7   Month_Record  21866 non-null  int64 \n 8   Day_Record    21866 non-null  int64 \n 9   Day_of_Week   21866 non-null  int64 \n 10  Hour_00       21866 non-null  int64 \n 11  Hour_01       21866 non-null  int64 \n 12  Hour_02       21866 non-null  int64 \n 13  Hour_03       21866 non-null  int64 \n 14  Hour_04       21866 non-null  int64 \n 15  Hour_05       21866 non-null  int64 \n 16  Hour_06       21866 non-null  int64 \n 17  Hour_07       21866 non-null  int64 \n 18  Hour_08       21866 non-null  int64 \n 19  Hour_09       21866 non-null  int64 \n 20  Hour_10       21866 non-null  int64 \n 21  Hour_11       21866 non-null  int64 \n 22  Hour_12       21866 non-null  int64 \n 23  Hour_13       21866 non-null  int64 \n 24  Hour_14       21866 non-null  int64 \n 25  Hour_15       21866 non-null  int64 \n 26  Hour_16       21866 non-null  int64 \n 27  Hour_17       21866 non-null  int64 \n 28  Hour_18       21866 non-null  int64 \n 29  Hour_19       21866 non-null  int64 \n 30  Hour_20       21866 non-null  int64 \n 31  Hour_21       21866 non-null  int64 \n 32  Hour_22       21866 non-null  int64 \n 33  Hour_23       21866 non-null  int64 \n 34  Restrictions  21866 non-null  int64 \ndtypes: int64(34), object(1)\nmemory usage: 5.8+ MB\nNone\n\n\nThe traffic data above is sourced from the Federal Highway Administration (FHWA) of the U.S. Department of Transportation (https://www.fhwa.dot.gov/policyinformation/tables/tmasdata/). It will serve as a metric for urbanization level of regions throughout Arizona. It is presumed that counties with higher traffic flow have greater urbanization than counties with low traffic flow.\nThe traffic data are sourced from two different file structures, counts data and station data.\n\nThe station data contains information about sampling locations including latitude and longitude, number of lanes on road, and type of sensor. Our main variables of interest will be latitude (numerical), longitude (numerical), and county (categorical)\nThe count data is nearly all numerical. Each row is a station location on a specific day of the given file month/year. Each row includes counts of passing vehicles at the listed station location for every hour of that day. We will engineer a feature variable aggregating total counts over a year at each station location. Variables of interest include the hourly counts (ex. Hour_05) and the station number."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Urbanization and Environmental Quality in Arizona, USA",
    "section": "Questions",
    "text": "Questions\nThe two questions you want to answer.\n\nCan we predict environmental quality of a region based on urbanization indicators for that region?\nAre storm event data and traffic data successful environmental health and urbanization indicators, respectively?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Urbanization and Environmental Quality in Arizona, USA",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nWe will identify the relationship between traffic data, the metric for urbanization, and weather and storm data, the metrics for environmental quality. We will do this with multivariate regression models, with a hypothesis that there is a negative linear relationship between urbanization and environmental quality.\nWe will conduct a PCA of the data by region to compare urbanized and rural counties within the state of Arizona.\nTime permitting, we will conduct a temporal analysis over a 5 year period to see how each region compares to itself in different years."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Title",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Project Title",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by JKP For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nVera Jackson\nMolly Kerwick\nBrooke Pacheco"
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Project title",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Project title",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Project title",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Thu, 07 Aug 2025   Prob (F-statistic):           5.84e-08\nTime:                        00:31:12   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Project title",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Project title",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Project title",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Project title",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Project title",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Project title",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  }
]